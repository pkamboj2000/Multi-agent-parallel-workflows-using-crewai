{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667e140a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18da65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Crew\n",
    "from crewai.flow.flow import Flow, start, listen  \n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6355dd35",
   "metadata": {},
   "source": [
    "## Load the  LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed4ff090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M4) - 10916 MiB free\n",
      "gguf_init_from_file_impl: invalid magic characters: 'Entr', expected 'GGUF'\n",
      "llama_model_load: error loading model: llama_model_loader: failed to load model from llama-3-8b-function-calling.Q4_K_M.gguf\n",
      "llama_model_load_from_file_impl: failed to load model\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatLlamaCpp\n  Value error, Could not load Llama model from path: llama-3-8b-function-calling.Q4_K_M.gguf. Received error Failed to load model from file: llama-3-8b-function-calling.Q4_K_M.gguf [type=value_error, input_value={'model_path': 'llama-3-8...': 0.0, 'verbose': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllamacpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatLlamaCpp\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatLlamaCpp(\n\u001b[1;32m      4\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3-8b-function-calling.Q4_K_M.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     n_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      6\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      7\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m      8\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_core/load/serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(data, self_instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    260\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatLlamaCpp\n  Value error, Could not load Llama model from path: llama-3-8b-function-calling.Q4_K_M.gguf. Received error Failed to load model from file: llama-3-8b-function-calling.Q4_K_M.gguf [type=value_error, input_value={'model_path': 'llama-3-8...': 0.0, 'verbose': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models.llamacpp import ChatLlamaCpp\n",
    "\n",
    "llm = ChatLlamaCpp(\n",
    "    model_path=\"llama-3-8b-function-calling.Q4_K_M.gguf\",\n",
    "    n_ctx=1024,\n",
    "    n_threads=4,\n",
    "    temperature=0.0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec0def",
   "metadata": {},
   "source": [
    "## Load Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "@tool(\"DuckDuckGo Search\")\n",
    "def search_tool1(query: str) -> str:\n",
    "    \"\"\"Searches the web using DuckDuckGo.\"\"\"\n",
    "    return DuckDuckGoSearchRun().run(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046246b",
   "metadata": {},
   "source": [
    "## Agent Definitions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_agent = Agent(\n",
    "    role=\"Loader Agent\",\n",
    "    goal=\"Search and retrieve electric consumption data for Indian states (2018-2023).\",\n",
    "    backstory=\"A research assistant who finds credible data sources from the web.\",\n",
    "    tools=[search_tool1],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "\n",
    "@tool(\"analyzer\")\n",
    "def search_tool2(query: str) -> str:\n",
    "    \"\"\"Searches the web using DuckDuckGo.\"\"\"\n",
    "    return PythonREPLTool().run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4329ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_agent = Agent(\n",
    "    role=\"Analyzer Agent\",\n",
    "    goal=\"Analyze the CSV to find top 3 states with highest average consumption.\",\n",
    "    backstory=\"A data analyst who uses pandas to extract patterns.\",\n",
    "    tools=[search_tool1],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "#This one loads CSV and analyzes with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer_agent = Agent(\n",
    "    role=\"Visualizer Agent\",\n",
    "    goal=\"Create bar charts to show electricity consumption per year by state.\",\n",
    "    backstory=\"A matplotlib expert for Indian state electricity data.\",\n",
    "    tools=[search_tool2],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "#Makes visual plots of electricity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aef407",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_agent = Agent(\n",
    "    role=\"Summarizer Agent\",\n",
    "    goal=\"Write a 2-3 sentence summary about which states consumed the most electricity and why.\",\n",
    "    backstory=\"A professional executive summary writer.\",\n",
    "    tools=[],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "#Pure LLM-based summary writer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6543280",
   "metadata": {},
   "source": [
    "## Task Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_task = Task(\n",
    "    description=\"Search for electricity consumption datasets for Indian states (2018–2023).\",\n",
    "    agent=loader_agent,\n",
    "    expected_output=\"A list of 3 URLs to government or trusted sources\",\n",
    "    input_type=\"input\"\n",
    "    )\n",
    "\n",
    "\n",
    "analyze_task = Task(\n",
    "    description=\"Load a CSV file, calculate average consumption, and print top 3 states.\",\n",
    "    agent=analyzer_agent,\n",
    "    expected_output=\"A list of the top 3 Indian states with the highest average electricity consumption.\"\n",
    ")\n",
    "\n",
    "visualize_task = Task(\n",
    "    description=\"Use matplotlib/seaborn to plot electricity consumption for each state.\",\n",
    "    agent=visualizer_agent,\n",
    "    expected_output=\"A bar chart or line graph showing electricity consumption for each Indian state.\"\n",
    ")\n",
    "\n",
    "summarize_task = Task(\n",
    "    description=\"Write a summary of insights from the data and visualizations in 2-3 sentences.\",\n",
    "    agent=summarizer_agent,\n",
    "    expected_output=\"A concise textual summary of the most important insights from the analysis and visualizations.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0391ffa4",
   "metadata": {},
   "source": [
    "## Flow Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f73ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">╭─────────────────────────────────────────────── 🤖 Agent Started ────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">│</span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\">│</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">Loader Agent</span>                                                                                            <span style=\"color: #800080; text-decoration-color: #800080\">│</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">│</span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\">│</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Task: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Search for electricity consumption datasets for Indian states (2018–2023).</span>                               <span style=\"color: #800080; text-decoration-color: #800080\">│</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">│</span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\">│</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m╭─\u001b[0m\u001b[35m──────────────────────────────────────────────\u001b[0m\u001b[35m 🤖 Agent Started \u001b[0m\u001b[35m───────────────────────────────────────────────\u001b[0m\u001b[35m─╮\u001b[0m\n",
       "\u001b[35m│\u001b[0m                                                                                                                 \u001b[35m│\u001b[0m\n",
       "\u001b[35m│\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[1;92mLoader Agent\u001b[0m                                                                                            \u001b[35m│\u001b[0m\n",
       "\u001b[35m│\u001b[0m                                                                                                                 \u001b[35m│\u001b[0m\n",
       "\u001b[35m│\u001b[0m  \u001b[37mTask: \u001b[0m\u001b[92mSearch for electricity consumption datasets for Indian states (2018–2023).\u001b[0m                               \u001b[35m│\u001b[0m\n",
       "\u001b[35m│\u001b[0m                                                                                                                 \u001b[35m│\u001b[0m\n",
       "\u001b[35m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=verbose=False client=<llama_cpp.llama.Llama object at 0x156cea630> model_path='/Users/pranjalkamboj/Desktop/projects/multi-agents-and-tools-using-Crewai/openchat-3.5-0106.Q4_K_M.gguf' n_ctx=1024 n_threads=4 model_kwargs={}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m loader_crew \u001b[38;5;241m=\u001b[39m Crew(\n\u001b[1;32m      5\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[loader_agent],\n\u001b[1;32m      6\u001b[0m     tasks\u001b[38;5;241m=\u001b[39m[load_task],\n\u001b[1;32m      7\u001b[0m     process\u001b[38;5;241m=\u001b[39mProcess\u001b[38;5;241m.\u001b[39msequential\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#  Get the context (output from loader_agent)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m context \u001b[38;5;241m=\u001b[39m loader_crew\u001b[38;5;241m.\u001b[39mkickoff(inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch for electricity consumption datasets for Indian states (2018–2023). Focus on government or trusted sources. Return top 3 links.\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#  Step 2: Set the next tasks to run in parallel\u001b[39;00m\n\u001b[1;32m     16\u001b[0m analyze_task\u001b[38;5;241m.\u001b[39masync_execution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/crew.py:669\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_crew_planning()\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 669\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sequential_process()\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[1;32m    671\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/crew.py:780\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[1;32m    779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_tasks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/crew.py:883\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    880\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    882\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[0;32m--> 883\u001b[0m task_output \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mexecute_sync(\n\u001b[1;32m    884\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent_to_use,\n\u001b[1;32m    885\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    886\u001b[0m     tools\u001b[38;5;241m=\u001b[39mcast(List[BaseTool], tools_for_task),\n\u001b[1;32m    887\u001b[0m )\n\u001b[1;32m    888\u001b[0m task_outputs\u001b[38;5;241m.\u001b[39mappend(task_output)\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/task.py:356\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    351\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    352\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    353\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_core(agent, context, tools)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/task.py:504\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    503\u001b[0m crewai_event_bus\u001b[38;5;241m.\u001b[39memit(\u001b[38;5;28mself\u001b[39m, TaskFailedEvent(error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e), task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m--> 504\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/task.py:420\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[1;32m    419\u001b[0m crewai_event_bus\u001b[38;5;241m.\u001b[39memit(\u001b[38;5;28mself\u001b[39m, TaskStartedEvent(context\u001b[38;5;241m=\u001b[39mcontext, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m--> 420\u001b[0m result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mexecute_task(\n\u001b[1;32m    421\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    422\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    423\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    424\u001b[0m )\n\u001b[1;32m    426\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[1;32m    427\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[1;32m    428\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    429\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[1;32m    436\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agent.py:462\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     crewai_event_bus\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    456\u001b[0m         event\u001b[38;5;241m=\u001b[39mAgentExecutionErrorEvent(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    460\u001b[0m         ),\n\u001b[1;32m    461\u001b[0m     )\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agent.py:438\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    434\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_with_timeout(\n\u001b[1;32m    435\u001b[0m             task_prompt, task, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_execution_time\n\u001b[1;32m    436\u001b[0m         )\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_without_timeout(task_prompt, task)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# Propagate TimeoutError without retry\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     crewai_event_bus\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    444\u001b[0m         event\u001b[38;5;241m=\u001b[39mAgentExecutionErrorEvent(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m         ),\n\u001b[1;32m    449\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agent.py:534\u001b[0m, in \u001b[0;36mAgent._execute_without_timeout\u001b[0;34m(self, task_prompt, task)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_without_timeout\u001b[39m(\u001b[38;5;28mself\u001b[39m, task_prompt: \u001b[38;5;28mstr\u001b[39m, task: Task) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    525\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute a task without a timeout.\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m        The output of the agent.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m    535\u001b[0m         {\n\u001b[1;32m    536\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: task_prompt,\n\u001b[1;32m    537\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_names\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_names,\n\u001b[1;32m    538\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_description,\n\u001b[1;32m    539\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: task\u001b[38;5;241m.\u001b[39mhuman_input,\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m     )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:114\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_loop()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[1;32m    117\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    118\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:208\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_context_length_exceeded(e):\n\u001b[1;32m    210\u001b[0m         handle_context_length(\n\u001b[1;32m    211\u001b[0m             respect_context_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrespect_context_window,\n\u001b[1;32m    212\u001b[0m             printer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m             i18n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i18n,\n\u001b[1;32m    217\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:154\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     formatted_answer \u001b[38;5;241m=\u001b[39m handle_max_iterations_exceeded(\n\u001b[1;32m    144\u001b[0m         formatted_answer,\n\u001b[1;32m    145\u001b[0m         printer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    152\u001b[0m enforce_rpm_limit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit)\n\u001b[0;32m--> 154\u001b[0m answer \u001b[38;5;241m=\u001b[39m get_llm_response(\n\u001b[1;32m    155\u001b[0m     llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm,\n\u001b[1;32m    156\u001b[0m     messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages,\n\u001b[1;32m    157\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m    158\u001b[0m     printer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer,\n\u001b[1;32m    159\u001b[0m     from_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\n\u001b[1;32m    160\u001b[0m )\n\u001b[1;32m    161\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m process_llm_response(answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_stop_words)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Extract agent fingerprint if available\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/utilities/agent_utils.py:160\u001b[0m, in \u001b[0;36mget_llm_response\u001b[0;34m(llm, messages, callbacks, printer, from_task, from_agent)\u001b[0m\n\u001b[1;32m    153\u001b[0m     answer \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m    154\u001b[0m         messages,\n\u001b[1;32m    155\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    156\u001b[0m         from_task\u001b[38;5;241m=\u001b[39mfrom_task,\n\u001b[1;32m    157\u001b[0m         from_agent\u001b[38;5;241m=\u001b[39mfrom_agent,\n\u001b[1;32m    158\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n\u001b[1;32m    162\u001b[0m     printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[1;32m    163\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived None or empty response from LLM call.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    164\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    165\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/utilities/agent_utils.py:153\u001b[0m, in \u001b[0;36mget_llm_response\u001b[0;34m(llm, messages, callbacks, printer, from_task, from_agent)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     answer \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m    154\u001b[0m         messages,\n\u001b[1;32m    155\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    156\u001b[0m         from_task\u001b[38;5;241m=\u001b[39mfrom_task,\n\u001b[1;32m    157\u001b[0m         from_agent\u001b[38;5;241m=\u001b[39mfrom_agent,\n\u001b[1;32m    158\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/llm.py:971\u001b[0m, in \u001b[0;36mLLM.call\u001b[0;34m(self, messages, tools, callbacks, available_functions, from_task, from_agent)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_streaming_response(\n\u001b[1;32m    968\u001b[0m             params, callbacks, available_functions, from_task, from_agent\n\u001b[1;32m    969\u001b[0m         )\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_non_streaming_response(\n\u001b[1;32m    972\u001b[0m             params, callbacks, available_functions, from_task, from_agent\n\u001b[1;32m    973\u001b[0m         )\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LLMContextLengthExceededException:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Re-raise LLMContextLengthExceededException as it should be handled\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# by the CrewAgentExecutor._invoke_loop method, which can then decide\u001b[39;00m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# whether to summarize the content or abort based on the respect_context_window flag\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/crewai/llm.py:781\u001b[0m, in \u001b[0;36mLLM._handle_non_streaming_response\u001b[0;34m(self, params, callbacks, available_functions, from_task, from_agent)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# --- 1) Make the completion call\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# Attempt to make the completion call, but catch context window errors\u001b[39;00m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# and convert them to our own exception type for consistent handling\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# across the codebase. This allows CrewAgentExecutor to handle context\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# length issues appropriately.\u001b[39;00m\n\u001b[0;32m--> 781\u001b[0m     response \u001b[38;5;241m=\u001b[39m litellm\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ContextWindowExceededError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Convert litellm's context window error to our own exception type\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# for consistent handling in the rest of the codebase\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LLMContextLengthExceededException(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/utils.py:1306\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1303\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1304\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1305\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1306\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/utils.py:1181\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1181\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1182\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[1;32m   1184\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   1185\u001b[0m     call_type\u001b[38;5;241m=\u001b[39mcall_type,\n\u001b[1;32m   1186\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/main.py:3430\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   3427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   3428\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3429\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[1;32m   3431\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   3432\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   3433\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   3434\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   3435\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   3436\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/main.py:1100\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     model \u001b[38;5;241m=\u001b[39m deployment_id\n\u001b[1;32m   1099\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1100\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m get_llm_provider(\n\u001b[1;32m   1101\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1102\u001b[0m     custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   1103\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m   1104\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   1105\u001b[0m )\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1108\u001b[0m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_llm_provider\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m custom_llm_provider\n\u001b[1;32m   1110\u001b[0m ):\n\u001b[1;32m   1111\u001b[0m     headers\u001b[38;5;241m.\u001b[39mupdate(provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:379\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[0;32m--> 379\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m         error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    382\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:356\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    354\u001b[0m     error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Pass model as E.g. For \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuggingface\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m inference endpoints pass in `completion(model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/starcoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    357\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    358\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    359\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[1;32m    360\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m    361\u001b[0m             content\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    362\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    363\u001b[0m         ),\n\u001b[1;32m    364\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    365\u001b[0m     )\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(api_base)\n\u001b[1;32m    369\u001b[0m     )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=verbose=False client=<llama_cpp.llama.Llama object at 0x156cea630> model_path='/Users/pranjalkamboj/Desktop/projects/multi-agents-and-tools-using-Crewai/openchat-3.5-0106.Q4_K_M.gguf' n_ctx=1024 n_threads=4 model_kwargs={}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "from crewai import Crew, Process\n",
    "\n",
    "#  Step 1: Run the loader agent first (sequential)\n",
    "loader_crew = Crew(\n",
    "    agents=[loader_agent],\n",
    "    tasks=[load_task],\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "#  Get the context (output from loader_agent)\n",
    "context = loader_crew.kickoff(inputs={\"input\": \"Search for electricity consumption datasets for Indian states (2018–2023). Focus on government or trusted sources. Return top 3 links.\"})\n",
    "\n",
    "\n",
    "\n",
    "#  Step 2: Set the next tasks to run in parallel\n",
    "analyze_task.async_execution = True\n",
    "visualize_task.async_execution = True\n",
    "summarize_task.async_execution = True\n",
    "\n",
    "#  Step 3: Run the 3 agents in parallel using shared context\n",
    "main_crew = Crew(\n",
    "    agents=[analyzer_agent, visualizer_agent, summarizer_agent],\n",
    "    tasks=[analyze_task, visualize_task, summarize_task],\n",
    "    process=Process.parallel\n",
    ")\n",
    "\n",
    "#  Execute the second crew and collect results\n",
    "final_result = main_crew.kickoff(context=context)\n",
    "\n",
    "# Print the final combined output\n",
    "print(final_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c72ba",
   "metadata": {},
   "source": [
    "##  Run and Show Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f8ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = Crew(flow=flow, verbose=True)\n",
    "result = crew.kickoff()\n",
    "\n",
    "print(\"\\n Final Result:ssssss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6c051",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
